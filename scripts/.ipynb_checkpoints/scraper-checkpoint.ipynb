{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_assistant import creat_company_url, get_request, main_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(cik):\n",
    "    \n",
    "    # find the 13f document webpage of a company/fund by a given CIK number\n",
    "    \n",
    "    time.sleep(0.15)\n",
    "    \n",
    "    response = get_request(create_company_url(cik))\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\", parse_only=SoupStrainer(['a','span','input']))\n",
    "\n",
    "    # find the 13F document tags of the company, and \n",
    "    # the tag contains the link to the detailed filing pages\n",
    "\n",
    "    tags = soup.find_all('a', id=\"documentsbutton\")\n",
    "\n",
    "    # check if the company has 13f forms\n",
    "    if len(tags)==0:\n",
    "        #statement = \"No 13f files found for this company\"\n",
    "        return False\n",
    "\n",
    "    else:\n",
    "        # retrieve name and cik information of the company\n",
    "        company_name_text = soup.find('span', {'class': \"companyName\"}).get_text()\n",
    "        company_name = re.sub(' CIK#.*$', '', company_name_text)\n",
    "        company_cik_text = soup.find('input', {'name':'CIK'})\n",
    "        company_cik = 'cik_number_' + company_cik_text.get('value')\n",
    "        \n",
    "        # scrape the detailed webpage of each 13f file\n",
    "        for tag in tags:\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            response_1 = get_request(main_url+tag['href'])\n",
    "            soup_1 = BeautifulSoup(response_1.text, \"lxml\",parse_only=SoupStrainer(['div','strong','a']))\n",
    "            \n",
    "            # find the tag that contains the information table\n",
    "            holding_tag = soup_1.find('a', href=lambda x: x and 'xslForm13F_X01'in x and not 'primary_doc' in x)\n",
    "            \n",
    "            # if no target table found, stop this round of loop\n",
    "            if holding_tag is None:\n",
    "                continue\n",
    "            \n",
    "            # retrieve the period of report of the 13f form\n",
    "            time_tags = soup_1.find_all('div', class_=\"info\")\n",
    "            len_time = len(time_tags)\n",
    "            if len_time < 5:\n",
    "                continue\n",
    "                \n",
    "            period_of_report = time_tags[-2].get_text()\n",
    "\n",
    "            # retrieve the file type of the 13f form, in case it is an amendment\n",
    "            file_type_raw = soup_1.find('strong').get_text()\n",
    "            file_type = file_type_raw.replace('/','-')\n",
    "            \n",
    "\n",
    "            try:\n",
    "                # get the link to the information table page\n",
    "                xml_url = holding_tag.get('href')\n",
    "                \n",
    "                # parse the information table page\n",
    "                file_response = get_request(main_url+xml_url)\n",
    "                \n",
    "                HR_df = pd.read_html(file_response.text)\n",
    "                HR_df = HR_df[-1]# the target table is the last one in the page\n",
    "                # adjust the format of the dataframe\n",
    "                HR_df = HR_df.iloc[2:]\n",
    "                header_new = HR_df.iloc[0].tolist()\n",
    "                header_new[3] = 'VALUE ' + header_new[3]\n",
    "                header_new[4] = 'SHRS OR ' + header_new[4]\n",
    "                header_new[5] = 'SH/' + header_new[5]\n",
    "                header_new[6] = 'PUT/' + header_new[6]\n",
    "                header_new[7] = 'INVESTMENT ' + header_new[7]\n",
    "                header_new[8] = 'OTHER ' + header_new[8]\n",
    "                #header_new[9] = 'VOTING AUTHORITY '+ header_new[9]\n",
    "                #header_new[10]= 'VOTING AUTHORITY '+ header_new[10]\n",
    "                #header_new[11]= 'VOTING AUTHORITY '+ header_new[11]\n",
    "                HR_df.columns = header_new\n",
    "                \n",
    "                HR_df = HR_df.iloc[1:]\n",
    "                \n",
    "                HR_df['Company cik'] = company_cik\n",
    "                HR_df['Company name'] = company_name\n",
    "                HR_df['Period of report'] = period_of_report\n",
    "                HR_df['File type'] = file_type\n",
    "                \n",
    "                # export the dataframe as a csv file to the data directory\n",
    "                df_name = company_cik +'_'+ file_type +'_'+ period_of_report\n",
    "                HR_df.to_csv(f\"../data/13f/{df_name}.csv\".format(df_name))\n",
    "\n",
    "            except:\n",
    "                print ('error')\n",
    "                pass\n",
    "        \n",
    "        return True\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
